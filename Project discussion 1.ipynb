{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions of some confusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "orient='h' means boxplot is horizontal, palette=\"Set2\" means colors\n",
    "\n",
    "normally distributed= normal curve\n",
    "\n",
    "80% relations are on weight and size.That's why weight and height are most important\n",
    "\n",
    "threshold =3 means all the values between -3 to +3 will be kept and others will be removed\n",
    "\n",
    "x= we are dropping weight from x variable.Other columns will be there except weight\n",
    "\n",
    "y=we are selecting weight as a target variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HeadBrain use case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Gender : 1 for male, 2 for female\n",
    "    \n",
    "2. Age range : 1 for ages 20-46, 2 for over 46\n",
    "    \n",
    "3. Head size: head size is cubic cm\n",
    "    \n",
    "4.Brain Weighr : brain weight is grams  \n",
    "    \n",
    "Our target variable is 'Brain Weight' which has continuous data . So, it's a regression problem (continuous=regression, catagorical=classification)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing warning library to avoid any warning message\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain=pd.read_csv('headbrain.csv')\n",
    "brain.head() #printing first 5 rows for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.shape #Checking data dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q : Should I remove the outlier:\n",
    "        \n",
    "A : It needs to solve the problem using with outliers and without outliers. The best result will be accepted       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the column names to bit easier terms\n",
    "brain=brain.rename(columns={'AgeRange':'Age','HeadSize':'Size','BrainWeight':'Weight'})\n",
    "brain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.describe() #ststistical summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Here we can see there are no missing values\n",
    "\n",
    "2. No much difference observed between mean and median so outliers are not there. Still we will validate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.info()  #checking the datatype of all the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no catagorical column exists in this dataset. So, no encoding is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.isnull().sum()  #Checking if there is any null values or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null value in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate analysis\n",
    "\n",
    "Analysis the data for one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distplot(brain[\"Size\"])\n",
    "plt.title(\"Brain size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is showing that data is normally distributed in size column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(brain[\"Weight\"])\n",
    "plt.title(\"Brain weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is showing that data is normally distributed in size column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the \"Gender\" column\n",
    "sns.countplot(brain[\"Gender\"])    #There are only two values.Thats's why we have used countplot.\n",
    "plt.title(\"Age Range\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the \"Age\" column\n",
    "sns.countplot(brain[\"Age\"])    #There are only two values.Thats's why we have used countplot.\n",
    "plt.title(\"Age Range\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data distribution of \"Age Range\" column is showing normal between both the catagories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Analysis (two columns are for analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting scatter plot to check relationship between \"Size\" and \"Weight\"\n",
    "plt.scatter(brain[\"Size\"],brain[\"Weight\"])\n",
    "plt.title(\"Scatter plot\")\n",
    "plt.xlabel(\"Size\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is positive linear relation above the scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking correlation using heatmap\n",
    "corr=brain.corr()  #brain.corr() function provies the correlation value of each column\n",
    "sns.heatmap(corr,annot=True) #Using heatmap we are plotting correlation values obtained by brain.corr() function\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Very low correlation of \"Age\" with target variable so we can drop this column\n",
    "\n",
    "2.Very high and positive correlation of \"Size\" with the target variable\n",
    "\n",
    "3.Very negative correlation of \"Gender\" column with target variable but we will keep it as relation is strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers can impact the mean significantly. So it's always better to check the dataset for outliers and treat them if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking outliers using boxplot\n",
    "sns.boxplot(data=brain[[\"Size\",\"Weight\"]],orient=\"h\",palette=\"Set2\")\n",
    "plt.title(\"Outliers detection using Boxplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are outliers which can be removed using different methods. In this project we will use Inter Quartile Range to remove the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "z=np.abs(zscore(brain))\n",
    "threshold=3\n",
    "new_brain=brain[(z<3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape=\",brain.shape,\"\\n\",\"New shape : \", new_brain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewness is also an important factor which impacts distribution of the data.Always try to make the dataset less skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the skewness(if exists)\n",
    "new_brain.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness is in acceptable range (+/- 0.5) so our data is not skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating independent and target variable\n",
    "x=new_brain.drop(\"Weight\",axis=1)\n",
    "y=new_brain[\"Weight\"] #Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model can be bised to higher values in dataset so it's betetr to scale the dataset so that we can bring all the column in common range. There are two algorithms available for scaling,StandardScaler and MinMaxScaler. We are going to use StandardScaler here  (MinMax is for range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the dataset using SrandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "scaledx=sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#findinfg the best random state\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "best_rstate=0\n",
    "accu=0\n",
    "for i in range(30,200):\n",
    "    x_train,x_test,y_train,y_test=train_test_split(scaledx,y,test_size=.25,random_state=i)\n",
    "    mod=LinearRegression()\n",
    "    mod.fit(x_train,y_train)\n",
    "    y_pred=mod.predict(x_test)\n",
    "    tempaccu=r2_score(y_test,y_pred)   #What is the r2_score for particular random state\n",
    "    if tempaccu>accu:\n",
    "        accu=tempaccu\n",
    "        best_rstate=i\n",
    "        \n",
    "print(f\"Best accuracy{accu*100} found on random_state{best_rstate}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found best random state 128 and will be using it in train_test_split in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train_test_split using best random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(scaledx,y,test_size=.25,random_state=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "LR=LinearRegression()\n",
    "LR.fit(x_train,y_train)\n",
    "y_pred=LR.predict(x_test)\n",
    "r2score=r2_score(y_test,y_pred)\n",
    "cvscore=cross_val_score(LinearRegression(),x_train,y_train,cv=5).mean()\n",
    "print(f\"Accuracy={r2score*100},cross_val_score={cvscore*100} & difference={(r2score*100)-(cvscore*100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RF=RandomForestRegressor()\n",
    "RF.fit(x_train,y_train)\n",
    "y_pred=RF.predict(x_test)\n",
    "r2score=r2_score(y_test,y_pred)\n",
    "cvscore=cross_val_score(RandomForestRegressor(),x_train,y_train,cv=5).mean()\n",
    "print(f\"Accuracy={r2score*100},cross_val_score={cvscore*100} & difference={(r2score*100)-(cvscore*100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ADB=AdaBoostRegressor()\n",
    "ADB.fit(x_train,y_train)\n",
    "y_pred=ADB.predict(x_test)\n",
    "r2score=r2_score(y_test,y_pred)\n",
    "cvscore=cross_val_score(AdaBoostRegressor(),x_train,y_train,cv=5).mean()\n",
    "print(f\"Accuracy={r2score*100},cross_val_score={cvscore*100} & difference={(r2score*100)-(cvscore*100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "SGD=SGDRegressor()\n",
    "SGD.fit(x_train,y_train)\n",
    "y_pred=SGD.predict(x_test)\n",
    "r2score=r2_score(y_test,y_pred)\n",
    "cvscore=cross_val_score(SGDRegressor(),x_train,y_train,cv=5).mean()\n",
    "print(f\"Accuracy={r2score*100},cross_val_score={cvscore*100} & difference={(r2score*100)-(cvscore*100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression and SGDRegressor are best performing model with almost same accuracy and cross valifdation score. We can choose either of them as our best model so I am moving ahead with SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's technique to find out the best parameter for our model to improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating parameter list to pass the GridSearchCV\n",
    "parameters = {\"max_iter\":[500,700,900,1100,1300,1400,1500],     #maximum iteration.It means how many times I will pass the data for iteration.\n",
    "              \n",
    "              \"alpha\":[0.0001,0.001,0.01,0.1,1,10,100],\n",
    "              \"penalty\":[\"elasticnet\",\"l1\",\"l2\"]}           #The best parameter will be selected automatically. We can use this code everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "GCV=GridSearchCV(SGDRegressor(),parameters,cv=5,scoring='r2')      #Initializing GridSearchCV cv=cross_validation ,r2 is for improving the accuracy\n",
    "GCV.fit(x_train,y_train)                                           #cv=5 means 5 cross folds will be created and there will be training and testing data . Any fold will not be empty.\n",
    "GCV.best_estimator_  #finding the best estimator\n",
    "GCV_pred=GCV.best_estimator_.predict(x_test) #Predicting the value using best estimator found by GridSearchCV\n",
    "print(\"Final accuracy : \",r2_score(y_test,GCV_pred)*100) #Final accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model accuracy is not increasing after 76.48%.It is because of less data. We need to collect more data to improve the accuracy further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(GCV.best_estimator_,\"HeadBrainProject.pkl\") #This trained model has been saved for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
